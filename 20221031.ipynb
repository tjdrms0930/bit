{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO5Oq9i2cM6t+U9cNw5+3i3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tjdrms0930/bit/blob/main/20221031.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qUCT2hxerTR"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import models, layers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = '해보지 않으면 해낼 수 없다.'\n",
        "result = text_to_word_sequence(text)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "XEFidEgAPHvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = ['먼저 텍스트의 각 단어를 나누어 토큰화 합니다.',\n",
        "       '텍스트의 단어로 토큰화해야 딥러닝에서 인식됩니다.',\n",
        "       '토큰화한 결과는 딥러닝에서 사용할 수 있습니다.',\n",
        "       ]"
      ],
      "metadata": {
        "id": "p8V1suP6PI1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token = Tokenizer()\n",
        "token.fit_on_texts(docs)"
      ],
      "metadata": {
        "id": "G7B57nHkPJzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(token.word_counts)"
      ],
      "metadata": {
        "id": "RQh66rVaPKud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(token.document_count) # 문장이 몇개인지 확인"
      ],
      "metadata": {
        "id": "yYPidg65PLmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(token.word_docs)"
      ],
      "metadata": {
        "id": "H0zLvUuuPMm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(token.word_index)"
      ],
      "metadata": {
        "id": "VFAkH6JaPS_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text='오랫동안 꿈꾸는 이는 그 꿈을 닮아간다'\n",
        "token = Tokenizer()\n",
        "token.fit_on_texts([text])\n",
        "print(token.word_index)"
      ],
      "metadata": {
        "id": "cDTThBs2QRno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = token.texts_to_sequences([text])\n",
        "print(x)"
      ],
      "metadata": {
        "id": "dovWHvIOQSmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_size = len(token.word_index) + 1\n",
        "x = to_categorical(x, num_classes=word_size)\n",
        "print(x)"
      ],
      "metadata": {
        "id": "OPCYG2uyQTpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = [\"너무 재밌네요\",\"최고예요\",\"참 잘 만든 영화예요\",\n",
        "        \"추천하고 싶은 영화입니다\",\"한번 더 보고싶네요\",\n",
        "        \"글쎄요\",\"별로예요\",\"생각보다 지루하네요\",\"연기가 어색해요\",\"재미없어요\"]\n",
        "classes = np.array([1,1,1,1,1,0,0,0,0,0])\n",
        "token = Tokenizer()\n",
        "token.fit_on_texts(docs)\n",
        "print(token.word_index)"
      ],
      "metadata": {
        "id": "7FDBJo9BRpeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = token.texts_to_sequences(docs)\n",
        "print(x)"
      ],
      "metadata": {
        "id": "bFGC11NvRs5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_x = pad_sequences(x, 4)\n",
        "print(padded_x)"
      ],
      "metadata": {
        "id": "ns1EFkBERx7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Embedding(len(token.word_index) + 1, 8, input_length=4))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "CQfCHStaSYQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(padded_x, classes, epochs=20)"
      ],
      "metadata": {
        "id": "259_TgfDVtE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizing"
      ],
      "metadata": {
        "id": "jes-THwhXKRZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 영어 토크나이징\n",
        "- NLTK(Natural Language Toolkit) 과 Spacy 가 대표적 임  \n",
        "## 1. NLTK  \n",
        "- 파이썬에서 영어 텍스트 전처리 작업을 하는 데 많이 쓰이는 라이브러리  \n",
        "- 50여개가 넘는 말뭉치(corpus) 리소스를 활용해 영어 텍스트를 분석할 수 있게 해줌  \n",
        "- 직관적인 함수 사용법으로 빠르게 텍스트 전처리를 할 수 있음"
      ],
      "metadata": {
        "id": "OD7CYeNjbY3-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "UII9HR3fVuSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('all-corpora')"
      ],
      "metadata": {
        "id": "dE2VLxoQba8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "2gTat1enbsQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "G--utl1pbtJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Natural language processing (NLP) is a subfield of computer science, \\\n",
        "information engineering, and artificial intelligence concerned with the interactions \\\n",
        "between computers and human (natural) languages, in particular how to program computers \\\n",
        "to process and analyze large amounts of natural language data.\""
      ],
      "metadata": {
        "id": "5x3JbjRHbuHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_tokenize(sentence))"
      ],
      "metadata": {
        "id": "CjB-7wF4bvKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "id": "RpDp9nAJcQoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph = \"Natural language processing (NLP) is a subfield of computer science, \\\n",
        "information engineering, and artificial intelligence concerned with the interactions \\\n",
        "between computers and human (natural) languages, in particular how to program computers \\\n",
        "to process and analyze large amounts of natural language data. Challenges in natural \\\n",
        "language processing frequently involve speech recognition, natural language \\\n",
        "understanding, and natural language generation.\""
      ],
      "metadata": {
        "id": "pu1iIDxlcRjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sent_tokenize(paragraph))"
      ],
      "metadata": {
        "id": "jfpq0_vMcS0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stopwords.words('english')[:20]"
      ],
      "metadata": {
        "id": "MByDPXPHcT96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(stopwords.words('english')))"
      ],
      "metadata": {
        "id": "SleIjOhxcU3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "word_tokens=word_tokenize(sentence)\n",
        "\n",
        "result = []\n",
        "for w in word_tokens:\n",
        "    if w not in stop_words:\n",
        "        result.append(w)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "Cz1iWDoWcVw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Spacy\n",
        "- 상업용 목적으로 만들어진 오픈소스 라이브러리  \n",
        "- 영어를 포함한 8개 국어에 대한 자연어 전처리 모듈을 제공  \n",
        "- 쉬운 설치 및 빠른 전처리\n",
        "- 버전오류로 통과"
      ],
      "metadata": {
        "id": "Dyt0TTIEc174"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 한글 토크나이징\n",
        "- 한글은 언어의 특성상 NLTK나 Spacy는 사용하기에 적합하지 않음  \n",
        "- 영어에 존재하지 않는 형태소 분석이나 음소 분리와 같은 내용은 다루기 어려움  \n",
        "- 한글 자연어 처리에 많이 사용되는 KoNLPy 에 대해 알아봄"
      ],
      "metadata": {
        "id": "AV3lD5IEdLXW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KoLNPy\n",
        "- 한글 자연어 처리를 위해 만들어진 오픈소스 라이브러리  \n",
        "- 국내에 이미 만들어져 사용되고 있는 여러 형태소 분석기를 사용할 수 있음  \n",
        "- 자바로 작성된 형태소 분석기를 사용하기 때문에 윈도우에서 KoNLPy를 설치하기 위해서는 Java(1.7 이상)가 필요  "
      ],
      "metadata": {
        "id": "nJQv4wT-e45N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import konlpy"
      ],
      "metadata": {
        "id": "vzmoApZugu5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 형태소 단위 토크나이징\n",
        "- KoNLPy에서는 여러 형태소 분석기를 제공  \n",
        "- 각 형태소 분석기는 클래스 형태로 되어 있고, 이를 객체로 생성한 후 메서드를 호출해서 토크나이징 함"
      ],
      "metadata": {
        "id": "ssSB41T6guNk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 형태소 분석 및 품사 태깅\n",
        "- 형태소란 의미를 가지는 가장 작은 단위  \n",
        "- KoNLPy에 객체 형태로 포함되어 있는 형태소 분석기 목록  \n",
        "a. Hannanum  \n",
        "b. Kkma  \n",
        "c. Komoran  \n",
        "d. Mecab  \n",
        "e. Okt(Twitter)  \n",
        "- 위 객체들은 모두 동일하게 형태소 분석 기능을 제공  \n",
        "- 각기 성능이 조금씩 다름  \n",
        "- Mecab은 윈도우에서 실행 불가능"
      ],
      "metadata": {
        "id": "8S8kEkYug-Qt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Hannanum\n",
        "from konlpy.tag import Kkma\n",
        "from konlpy.tag import Komoran\n",
        "from konlpy.tag import Okt"
      ],
      "metadata": {
        "id": "1dQ0YE0DcWh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "okt = Okt()\n",
        "text = \"한글 자연어 처리는 재밌다 이제부터 열심히 해야지ㅎㅎㅎ\"\n",
        "print(okt.morphs(text))"
      ],
      "metadata": {
        "id": "GlKRmuIfhEi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(okt.morphs(text, stem=True))"
      ],
      "metadata": {
        "id": "jmZJyIGHhmr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(okt.nouns(text))"
      ],
      "metadata": {
        "id": "UdDH-AsPhoEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(okt.phrases(text))"
      ],
      "metadata": {
        "id": "IepBoqEshpLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(okt.pos(text))\n",
        "print(okt.pos(text, join=True))"
      ],
      "metadata": {
        "id": "icvH3zCmhsMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kkma = Kkma()\n",
        "print(kkma.morphs(text))\n",
        "print(kkma.nouns(text))\n",
        "print(kkma.pos(text))"
      ],
      "metadata": {
        "id": "PU89dUXwiBOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "komoram = Komoran()\n",
        "print(komoram.morphs(text))\n",
        "print(komoram.nouns(text))\n",
        "print(komoram.pos(text))"
      ],
      "metadata": {
        "id": "3yg_zNG3iasn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hannanum = Hannanum()\n",
        "print(hannanum.morphs(text))\n",
        "print(hannanum.nouns(text))\n",
        "print(hannanum.pos(text))"
      ],
      "metadata": {
        "id": "AJ6_THjEigaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# kor_전처리"
      ],
      "metadata": {
        "id": "hTGUt7uZjYSc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud"
      ],
      "metadata": {
        "id": "1g6jqmeVisJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터 다운로드  \n",
        "1. 다운로드 주소  \n",
        "- https://github.com/e9t/nsmc  \n",
        "2. 파일 구성  \n",
        "- ratings.txt : 전체 리뷰를 모아둔 데이터, 전체 20만 개의 데이터로 구성됨  \n",
        "- ratings_train.txt : 학습 데이터, 총 15만 개의 데이터로 구성  \n",
        "- ratings_test.txt : 평가 데이터, 총 5만 개의 데이터로 구성"
      ],
      "metadata": {
        "id": "CORMxDsSkmOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_IN_PATH = './data-in/'\n",
        "train_data = pd.read_csv(DATA_IN_PATH + 'ratings_train.txt', delimiter='\\t',\n",
        "                        quoting=3) # quoting 문자데이터\n",
        "train_data.head()"
      ],
      "metadata": {
        "id": "j1NCKx9ekH9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('전체 학습데이터의 개수: {}'.format(len(train_data)))"
      ],
      "metadata": {
        "id": "PCtSfclVqQN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_length = train_data['document'].astype(str).apply(len)\n",
        "train_length.head()"
      ],
      "metadata": {
        "id": "dnxsQ6WiqRN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 그래프에 대한 이미지 사이즈 선언\n",
        "# figsize: (가로, 세로) 형태의 튜플로 입력\n",
        "plt.figure(figsize=(12, 5))\n",
        "# 히스토그램 선언\n",
        "# bins: 히스토그램 값들에 대한 버켓 범위\n",
        "# range: x축 값의 범위\n",
        "# alpha: 그래프 색상 투명도\n",
        "# color: 그래프 색상\n",
        "# label: 그래프에 대한 라벨\n",
        "plt.hist(train_length, bins=200, alpha=0.5, color= 'r')\n",
        "plt.yscale('log', nonpositive='clip')\n",
        "#non-positive values in y can be clipped to a very small positive number\n",
        "# 그래프 제목\n",
        "plt.title('Log-Histogram of length of review')\n",
        "# 그래프 x 축 라벨\n",
        "plt.xlabel('Length of review')\n",
        "# 그래프 y 축 라벨\n",
        "plt.ylabel('Number of review')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2A5qTTGkqSJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "# 박스플롯 생성\n",
        "# 첫번째 파라메터: 여러 분포에 대한 데이터 리스트를 입력\n",
        "# labels: 입력한 데이터에 대한 라벨\n",
        "# showmeans: 평균값을 마크함\n",
        "\n",
        "plt.boxplot(train_length, labels=['counts'], showmeans=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JrKu2A93q8iH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 길이가 긴 데이터가 꽤 존재함  \n",
        "- 중간값과 평균값은 아래쪽에 위치  \n",
        "- 워드클라우드를 이용해 자주 사용된 어휘 알아보기 "
      ],
      "metadata": {
        "id": "SFL2fM7uraFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_review = [review for review in train_data['document'] if type(review) is str]"
      ],
      "metadata": {
        "id": "xB1t8n5-q8vf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 워드 클라우드는 기본적으로 영어 텍스트를 지원함  \n",
        "- 한글을 사용하기 위해 한글 폰트를 설정해야 함  \n",
        "- 무료 한글폰트를 data-in 에 다운로드(나눔폰트)"
      ],
      "metadata": {
        "id": "-NKToqz2rnoZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wordclud = WordCloud(font_path = DATA_IN_PATH + 'NanumGothic.otf').generate(' '.join(train_review))\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.imshow(wordclud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "f6PKQN7jrbtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axe = plt.subplots(ncols=1)\n",
        "fig.set_size_inches(6, 3)\n",
        "sns.countplot(x=train_data['label'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4vl8mdbjsSre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"긍정 리뷰 개수: {}\".format(train_data['label'].value_counts()[1]))\n",
        "print(\"부정 리뷰 개수: {}\".format(train_data['label'].value_counts()[0]))"
      ],
      "metadata": {
        "id": "4gTkIxpTsnvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_word_counts = train_data['document'].astype(str).apply(lambda x:len(x.split(' ')))"
      ],
      "metadata": {
        "id": "n-OHNkDZstso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 10))\n",
        "plt.hist(train_word_counts, bins=50, facecolor='r',label='train')\n",
        "plt.title('Log-Histogram of word count in review', fontsize=15)\n",
        "plt.yscale('log', nonpositive='clip')\n",
        "plt.legend()\n",
        "plt.xlabel('Number of words', fontsize=15)\n",
        "plt.ylabel('Number of reviews', fontsize=15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ELkNRu4stPXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 길이의 경우 대부분 5개 정도에 분포되어 있음  \n",
        "- 30개 이상의 데이터부터는 수가 급격히 줄어듬  "
      ],
      "metadata": {
        "id": "2VjjZ3DVvHUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('리뷰 단어 개수 최대 값: {}'.format(np.max(train_word_counts)))\n",
        "print('리뷰 단어 개수 최소 값: {}'.format(np.min(train_word_counts)))\n",
        "print('리뷰 단어 개수 평균 값: {:.2f}'.format(np.mean(train_word_counts)))\n",
        "print('리뷰 단어 개수 표준편차: {:.2f}'.format(np.std(train_word_counts)))\n",
        "print('리뷰 단어 개수 중간 값: {}'.format(np.median(train_word_counts)))\n",
        "# 사분위의 대한 경우는 0~100 스케일로 되어있음\n",
        "print('리뷰 단어 개수 제 1 사분위: {}'.format(np.percentile(train_word_counts, 25)))\n",
        "print('리뷰 단어 개수 제 3 사분위: {}'.format(np.percentile(train_word_counts, 75)))"
      ],
      "metadata": {
        "id": "0V2mVvJdtbN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "평균 7 ~ 8 개 정도의 단어 수를 가지고 있고, 중간값의 경우 6개 정도의 단어를 가지고 있음  \n",
        "- 글자 수 제한때문에 영어 데이터에 비해 길이가 짧음  \n",
        "- 이 경우 모델에 적용할 최대 단어수를 6개가 아닌 7개로 설정해도 큰 무리가 없음  \n",
        "- 각 데이터에 대해 특수문자 유무를 확인  ㅠ\n",
        "- 리뷰에 자주 사용되는 특별한 특수문자는 없으므로 일반적인 마침표와 물음표만 확인"
      ],
      "metadata": {
        "id": "vjXHCjZbtxqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qmarks = np.mean(train_data['document'].astype(str).apply(lambda x: '?' in x)) # 물음표가 구두점으로 쓰임\n",
        "fullstop = np.mean(train_data['document'].astype(str).apply(lambda x: '.' in x)) # 마침표\n",
        "                  \n",
        "print('물음표가있는 질문: {:.2f}%'.format(qmarks * 100))\n",
        "print('마침표가 있는 질문: {:.2f}%'.format(fullstop * 100))"
      ],
      "metadata": {
        "id": "myrPx3pUtkUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_text = re.sub(\"[^가-힣ㄱ-ㅎㅏ-ㅣ\\\\s]\", \"\", train_data['document'][0])\n",
        "# 모든 한글, 자음, 모음, 공백을 제외한 모든 것을 삭제\n",
        "print(review_text)"
      ],
      "metadata": {
        "id": "v7X7ZtSntcMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "okt = Okt()\n",
        "review_text = okt.morphs(review_text, stem=True)\n",
        "print(review_text)"
      ],
      "metadata": {
        "id": "dPN5WOoJtwYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(['은', '는', '이', '가', '하', '아', '것', '들','의', '있', '되', '수', '보', '주', '등', '한'])\n",
        "clean_review = [token for token in review_text if not token in stop_words]\n",
        "print(clean_review)"
      ],
      "metadata": {
        "id": "GLrSvlGbvzbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(review, okt, remove_stopwords=False, stop_words=[]):\n",
        "    # 함수의 인자는 다음과 같다.\n",
        "    # review : 전처리할 텍스트\n",
        "    # okt : okt 객체를 반복적으로 생성하지 않고 미리 생성후 인자로 받는다.\n",
        "    # remove_stopword : 불용어를 제거할지 선택 기본값은 False\n",
        "    # stop_word : 불용어 사전은 사용자가 직접 입력해야함 기본값은 비어있는 리스트\n",
        "    \n",
        "    # 1. 한글 및 공백을 제외한 문자 모두 제거.\n",
        "    review_text = re.sub(\"[^가-힣ㄱ-ㅎㅏ-ㅣ\\\\s]\", \"\", review)\n",
        "    \n",
        "    # 2. okt 객체를 활용해서 형태소 단위로 나눈다.\n",
        "    word_review = okt.morphs(review_text, stem=True)\n",
        "    \n",
        "    if remove_stopwords:\n",
        "        \n",
        "        # 불용어 제거(선택적)\n",
        "        word_review = [token for token in word_review if not token in stop_words]\n",
        "        \n",
        "   \n",
        "    return word_review"
      ],
      "metadata": {
        "id": "gWxGZu0uwTwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_train_review = []\n",
        "\n",
        "for review in train_data['document']:\n",
        "    # 비어있는 데이터에서 멈추지 않도록 string인 경우만 진행\n",
        "    if type(review) == str:\n",
        "        clean_train_review.append(preprocessing(review, okt, remove_stopwords=True, stop_words=stop_words))\n",
        "    else:\n",
        "        clean_train_review.append([])  #string이 아니면 비어있는 값 추가\n",
        "        \n",
        "print(clean_train_review[:4])"
      ],
      "metadata": {
        "id": "kyGyLYV_w6T4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = pd.read_csv(DATA_IN_PATH + 'ratings_test.txt', delimiter='\\t', quoting=3 )\n",
        "\n",
        "clean_test_review = []\n",
        "\n",
        "for review in test_data['document']:\n",
        "    # 비어있는 데이터에서 멈추지 않도록 string인 경우만 진행\n",
        "    if type(review) == str:\n",
        "        clean_test_review.append(preprocessing(review, okt, remove_stopwords=True, stop_words=stop_words))\n",
        "    else:\n",
        "        clean_test_review.append([])  #string이 아니면 비어있는 값 추가\n",
        "        \n",
        "print(clean_test_review[:4])"
      ],
      "metadata": {
        "id": "4maLUv3Z2On-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(clean_train_review)\n",
        "train_sequences = tokenizer.texts_to_sequences(clean_train_review)\n",
        "test_sequences = tokenizer.texts_to_sequences(clean_test_review)\n",
        "\n",
        "word_vocab = tokenizer.word_index "
      ],
      "metadata": {
        "id": "f6ZARFFL2LYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_vocab['더빙'])"
      ],
      "metadata": {
        "id": "YRZt5JmH2z-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SEQUENCE_LENGTH = 8 # 문장 최대 길이, 단어의 평균 개수가 8개 정도였기 때문\n",
        "\n",
        "train_inputs = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post') # 학습 데이터를 벡터화\n",
        "train_labels = np.array(train_data['label']) # 학습 데이터의 라벨\n",
        "\n",
        "test_inputs = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post') # 테스트 데이터를 벡터화\n",
        "test_labels = np.array(test_data['label']) # 테스트 데이터의 라벨"
      ],
      "metadata": {
        "id": "b38pVXoi20O-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_INPUT_DATA = 'nsmc_train_input.npy'\n",
        "TRAIN_LABEL_DATA = 'nsmc_train_label.npy'\n",
        "TEST_INPUT_DATA = 'nsmc_test_input.npy'\n",
        "TEST_LABEL_DATA = 'nsmc_test_label.npy'\n",
        "DATA_CONFIGS = 'nsmc_data_configs.json'"
      ],
      "metadata": {
        "id": "PToJhPV53O4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_configs = {}\n",
        "\n",
        "data_configs['vocab'] = word_vocab\n",
        "data_configs['vocab_size'] = len(word_vocab) "
      ],
      "metadata": {
        "id": "lj9l96UM7rJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "if not os.path.exists(DATA_IN_PATH):\n",
        "    os.makedirs(DATA_IN_PATH)\n",
        "    \n",
        "# 전처리 된 학습 데이터를 넘파이 형태로 저장\n",
        "np.save(open(DATA_IN_PATH + TRAIN_INPUT_DATA, 'wb'), train_inputs)\n",
        "np.save(open(DATA_IN_PATH + TRAIN_LABEL_DATA, 'wb'), train_labels)\n",
        "# 전처리 된 테스트 데이터를 넘파이 형태로 저장\n",
        "np.save(open(DATA_IN_PATH + TEST_INPUT_DATA, 'wb'), test_inputs)\n",
        "np.save(open(DATA_IN_PATH + TEST_LABEL_DATA, 'wb'), test_labels)\n",
        "\n",
        "# 데이터 사전을 json 형태로 저장\n",
        "json.dump(data_configs, open(DATA_IN_PATH + DATA_CONFIGS, 'w'), ensure_ascii=False)"
      ],
      "metadata": {
        "id": "QnnmW61f7sGk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}